{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y llama-cpp-python llama_cpp_python\n",
        "!pip cache purge\n",
        "!pip install -U pip wheel\n",
        "\n",
        "!pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
        "!{sys.executable} -m pip install --force-reinstall --no-cache-dir \"numpy==2.1.3\"\n",
        "!pip install openpyxl\n",
        "!pip install --upgrade faiss-cpu\n",
        "!pip install -qU langchain-core\n",
        "!pip install -U \\\n",
        "  langchain-huggingface \\\n",
        "  \"sentence-transformers>=3.1\" \\\n",
        "  \"transformers>=4.44\" \\\n",
        "  \"huggingface_hub>=0.24\"\n",
        "# CUDA 12.4 wheels:\n",
        "!pip install --no-cache-dir --force-reinstall \\\n",
        "  --index-url https://abetlen.github.io/llama-cpp-python/whl/cu124 \\\n",
        "  --extra-index-url https://pypi.org/simple \\\n",
        "  \"llama-cpp-python==0.3.16\""
      ],
      "metadata": {
        "id": "deQMnLoUZXgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, llama_cpp\n",
        "print(\"py:\", sys.version)\n",
        "print(\"llama-cpp-python:\", llama_cpp.__version__)\n",
        "\n",
        "# Low-level C-extension handle (works across versions)\n",
        "try:\n",
        "    from llama_cpp import llama_cpp as _lib\n",
        "except Exception:\n",
        "    _lib = llama_cpp\n",
        "\n",
        "print(\"GPU offload support:\", getattr(_lib, \"llama_supports_gpu_offload\", lambda: \"unknown\")())\n",
        "\n",
        "bi = getattr(_lib, \"llama_build_info\", lambda: lambda: \"n/a\")()\n",
        "print(\"Build info:\", bi)\n"
      ],
      "metadata": {
        "id": "iocf0zztzeaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports (all local) ---\n",
        "import json\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Local LLM via Ollama\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "# Local embeddings + FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langgraph.graph import START, StateGraph\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time, random, json\n",
        "from collections import deque\n",
        "from google.colab import files\n",
        "from huggingface_hub import snapshot_download\n",
        "from langchain_community.chat_models import ChatLlamaCpp\n",
        "import pandas as pd, json, time, random, re\n",
        "from tqdm import tqdm\n",
        "from langchain_core.prompts import ChatPromptTemplate\n"
      ],
      "metadata": {
        "id": "pmGURyl-t55Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n"
      ],
      "metadata": {
        "id": "Dar8uYlXZTH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download both shards directly (resumable)\n",
        "local_dir = snapshot_download(\n",
        "    repo_id=\"Qwen/Qwen2.5-7B-Instruct-GGUF\",   # change if your repo is different\n",
        "    allow_patterns=[\n",
        "        \"qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf\",\n",
        "        \"qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf\"\n",
        "    ],\n",
        "    local_dir=\"/content/models/qwen2.5-7b-instruct\",\n",
        "    local_dir_use_symlinks=False # store real files\n",
        ")\n",
        "\n",
        "# Sanity check sizes\n",
        "!ls -lh /content/models/qwen2.5-7b-instruct"
      ],
      "metadata": {
        "id": "Tv89GEVF0roL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llm = ChatLlamaCpp(\n",
        "    model_path=\"/content/models/qwen2.5-7b-instruct/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf\",\n",
        "    n_ctx=8192,        # 4096 works; 8192 helps if your prompt is long\n",
        "    temperature=0.2,   # stable JSON\n",
        "    n_threads=4,       # CPU threads (tokenization etc.)\n",
        "    n_gpu_layers=-1,   # offload all layers that fit on the GPU\n",
        "    n_batch=256,      # <<< massive speedup for prompt evaluation\n",
        "    verbose=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "lC0uBWdWt7se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Local embeddings (multilingual bge-m3 or English bge-large-en-v1.5)\n",
        "#    Keep default settings; downloads once and then stays local.\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")"
      ],
      "metadata": {
        "id": "poRgupkGuBy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.schema import Document\n",
        "# If you're on LangChain >=0.1:\n",
        "from langchain_community.vectorstores import FAISS\n",
        "# else (older versions):\n",
        "# from langchain.vectorstores import FAISS\n",
        "\n",
        "# 1) Load labels (list[str])\n",
        "with open(\"label_names.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    labels = json.load(f)\n",
        "\n",
        "# 2) Make one Document per label (no splitting needed)\n",
        "docs = [\n",
        "    Document(page_content=lbl, metadata={\"label\": lbl, \"source\": \"label_names.json\"})\n",
        "    for lbl in labels\n",
        "]\n",
        "\n",
        "# 3) Build FAISS over labels\n",
        "# 'embeddings' should be your HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
        "vector_store = FAISS.from_documents(docs, embedding=embeddings)"
      ],
      "metadata": {
        "id": "L-kFd0TwuSDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'for i, sec in enumerate(guide.get(\"sections\", []), start=1):\n",
        "#     q = sec.get(\"question\", \"\").strip()\n",
        "#     a = sec.get(\"answer\", \"\").strip()\n",
        "\n",
        "#     # Optional: include structured contact info in the body to make it searchable\n",
        "#     extra = []\n",
        "#     cm = sec.get(\"contact_methods\")\n",
        "#     if cm:\n",
        "#         if \"email\" in cm: extra.append(f\"Email: {cm['email']}\")\n",
        "#         if \"phone\" in cm: extra.append(f\"Phone: {cm['phone']}\")\n",
        "#         live = cm.get(\"live_chat\")\n",
        "#         if live and \"availability\" in live:\n",
        "#             extra.append(f\"Live chat availability: {live['availability']}\")\n",
        "\n",
        "#     page_content = \"\\n\".join(\n",
        "#         part for part in [\n",
        "#             f\"Q: {q}\" if q else \"\",\n",
        "#             f\"A: {a}\" if a else \"\",\n",
        "#             \"\\n\".join(extra) if extra else \"\"\n",
        "#         ] if part\n",
        "#     )\n",
        "\n",
        "#     docs.append(\n",
        "#         Document(\n",
        "#             page_content=page_content,\n",
        "#             metadata={\n",
        "#                 \"section_index\": i,\n",
        "#                 \"question\": q,\n",
        "#                 \"title\": title,\n",
        "#                 \"version\": version,\n",
        "#                 \"source\": \"customer_support_guide_v1.json\",\n",
        "#             },\n",
        "#         )\n",
        "#     )'"
      ],
      "metadata": {
        "id": "tjcWemgRcxUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "custom_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"---------\",\n",
        "        \"Input:\\n{input}\\n\\nContext:\\n{context}\"\n",
        "    )\n",
        "])\n"
      ],
      "metadata": {
        "id": "xBI3aiCXQszI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "\n",
        "prompt = custom_prompt\n",
        "\n",
        "\n",
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    input: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "\n",
        "# Define application steps\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"input\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"input\": state[\"input\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "\n",
        "# Compile application and test\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "umQsjIZCToDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file as a DataFrame\n",
        "df = pd.read_excel(\"----\")  # add sheet_name=\"Sheet1\" if needed\n",
        "\n",
        "# Make a list of dicts â€“ one dict per row\n",
        "rows_as_dicts = df.to_dict(orient=\"records\")\n"
      ],
      "metadata": {
        "id": "wPcFgfvOl5ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []  # will hold the model answers\n",
        "for row_dict in rows_as_dicts:\n",
        "    # Build one combined string for the model to embed\n",
        "    combined_text = (\n",
        "        f\"x\t: {row_dict.get('x','')}\\n\"\n",
        "        f\"y: {row_dict.get('y','')}\\n\"\n",
        "        f\"z: {row_dict.get('z','')}\\n\"\n",
        "    )\n",
        "\n",
        "    # 4) Send the combined string to your graph\n",
        "    #    Change \"question\" to the key your graph expects\n",
        "    resp = graph.invoke({\"input\": combined_text})\n",
        "\n",
        "    # 5) Store the model's answer\n",
        "    results.append(resp.get(\"answer\", \"\"))\n",
        "\n",
        "# 6) Add model answers as a new column and save to Excel\n",
        "df[\"answer\"] = results\n",
        "df.to_excel(\"file.xlsx\", index=False)\n",
        "\n",
        "print(\"Finished. New file saved as file.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW7XiJ64mxmB",
        "outputId": "ce9a0ed7-a9ce-4b0d-fd4f-ef56f202e02b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished. New file saved as MA_open_with_answers.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import builtins\n",
        "\n",
        "# while True:\n",
        "#     user_input = builtins.input(\"Enter your question (or type 'exit' to quit): \")\n",
        "#     if user_input.lower() == \"exit\":\n",
        "#         break\n",
        "#     result_state = graph.invoke({\"user_input\": user_input})\n",
        "#     print(result_state[\"answer\"])"
      ],
      "metadata": {
        "id": "LQL5GEvMPRi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"file.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VLXmuffdotje",
        "outputId": "c04e87a3-9807-458b-cb82-59b63a6e1192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fd35ed6d-4de8-47fd-bb3f-042a9d952057\", \"MA_open_with_answers.xlsx\", 17002)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}